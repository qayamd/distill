# Model configuration
n_layer: 24
n_embd: 1024
n_experts: 8192
n_heads: 16
top_k: 16
d_key: 64
n_positions: 2048
share_params: true
model_name: "gpt2"
n_kv_heads: 16  

# Training configuration
learning_rate: 5e-5
warmup_steps: 2000
max_steps: 200000
grad_clip: 0.5
max_batch_size: 16
gradient_accumulation_steps: 4
T_0: 10000
T_mult: 2
weight_decay: 0.01  
model_sharding: true  
num_shards: 3
subset_size: 1000

# Mixed Precision and Optimization
use_mixed_precision: true
fp16_opt_level: "O2"

# Progressive Resizing
initial_sequence_length: 512
sequence_length_increase: 128
resize_interval: 5000

# Adaptive Layer Freezing
initial_frozen_layers: 18  
unfreeze_interval: 10000

# Model Pruning
pruning_interval: 20000
pruning_amount: 0.3  # Prune 30% of connections

# Layer-wise Distillation
layer_wise_loss_weight: 0.1
temperature: 2.0

# Dataset configuration
datasets:
  - numina_cot
  - gsm8k

num_epochs:
  numina_cot: 10 
  gsm8k: 2

# Paths
checkpoint_dir: './checkpoints'
log_dir: './logs'
tokenizer_path: './tokenizer.json'
tokenizer_config_path: './tokenizer_config.json'

# Teacher model configuration
teacher_model_name: "numina7b"
teacher_batch_size: 8

# Evaluation
eval_interval: 1000
save_interval: 5000

# Misc
seed: 42

# Data Loading
num_workers: 4
prefetch_factor: 2

# Logging
log_interval: 100
