# Model configuration
n_layer: 24
n_embd: 1024
n_experts: 16384
n_heads: 16
top_k: 16
d_key: 64
n_positions: 2048
share_params: true
model_name: "gpt2"
n_kv_heads: 16

# Training configuration
learning_rate: 1e-5
warmup_steps: 5000
max_steps: 50000
grad_clip: 0.5
max_batch_size: 32
gradient_accumulation_steps: 8
weight_decay: 0.01
model_sharding: true
num_shards: 3
subset_size: 10000
max_tokens_per_batch: 4096

# Optimizer
optimizer:
  type: "AdamW"
  betas: [0.9, 0.999]
  eps: 1e-8

# Scheduler
scheduler:
  type: "LambdaLR"

# Mixed Precision and Optimization
use_mixed_precision: true
precision: "float16"

# Progressive Resizing
initial_sequence_length: 512
sequence_length_increase: 128
resize_interval: 5000

# Adaptive Layer Freezing
initial_frozen_layers: 18
unfreeze_interval: 10000

# Model Pruning
pruning_interval: 20000
pruning_amount: 0.3

# Layer-wise Distillation
layer_wise_loss_weight: 0.1
temperature: 2.0

# Dataset configuration
datasets:
  - numina_cot
  - gsm8k
num_epochs:
  numina_cot: 3
  gsm8k: 2

# Paths
checkpoint_dir: './checkpoints'
log_dir: './logs'
tokenizer_path: './tokenizer.json'
tokenizer_config_path: './tokenizer_config.json'

# Teacher model configuration
teacher_model_name: "deepseek-math-7b-base"
teacher_batch_size: 8
teacher_model:
  files: 
    - "./model-00001-of-00003.safetensors"
    - "./model-00002-of-00003.safetensors"
    - "./model-00003-of-00003.safetensors"

# Evaluation
eval_interval: 1000
save_interval: 5000

# Misc
seed: 42
device: "cuda"

# Data Loading
dataloader:
  num_workers: 4
  pin_memory: true
prefetch_factor: 2

# Logging
logging:
  level: "INFO"
  log_to_file: true
  log_file: "training.log"
log_interval: 100

# Memory management
memory:
  clear_cache_interval: 1000

# Generation parameters
generation:
  max_length: 200
  num_return_sequences: 1
  no_repeat_ngram_size: 2
  do_sample: true
  top_k: 50
  top_p: 0.95
  temperature: 0.7

# Validation
validation:
  batch_size: 16
