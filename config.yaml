# Model configuration
n_layer: 24
n_embd: 1024
n_experts: 8192
n_heads: 16
top_k: 16
d_key: 64
n_positions: 2048
share_params: true
model_name: "gpt2"
n_kv_heads: 16  

# Training configuration
learning_rate: 5e-5  
warmup_steps: 2000 
max_steps: 200000  
gradient_accumulation_steps: 4  
grad_clip: 1.0 
max_batch_size: 16
gradient_accumulation_steps: 4
weight_decay: 0.01  
model_sharding: true  
num_shards: 3
subset_size: 1000
max_tokens_per_batch: 4096

# Optimizer
optimizer:
  type: "AdamW"
  betas: [0.9, 0.999]
  eps: 1e-8

# Scheduler
scheduler:
  type: "CosineAnnealingWarmRestarts"
  T_0_epochs: 1
  T_mult: 2

# Mixed Precision and Optimization
use_mixed_precision: true
precision: "float16"

# Progressive Resizing
initial_sequence_length: 512
sequence_length_increase: 128
resize_interval: 5000

# Adaptive Layer Freezing
initial_frozen_layers: 18  
unfreeze_interval: 10000

# Model Pruning
pruning_interval: 20000
pruning_amount: 0.3  # Prune 30% of connections

# Layer-wise Distillation
layer_wise_loss_weight: 0.1
temperature: 2.0

# Dataset configuration
datasets:
  - numina_cot
  - gsm8k
num_epochs:
  numina_cot: 10 
  gsm8k: 2

# Paths
checkpoint_dir: './checkpoints'
log_dir: './logs'
tokenizer_path: './tokenizer.json'
tokenizer_config_path: './tokenizer_config.json'

# Teacher model configuration
teacher_model_name: "numina7b"
teacher_batch_size: 8
teacher_model:
  files: ["./model-00001-of-00003.safetensors", 
          "./model-00002-of-00003.safetensors", 
          "./model-00003-of-00003.safetensors"]

# Evaluation
eval_interval: 1000
save_interval: 5000

# Misc
seed: 42
device: "cuda"

# Data Loading
dataloader:
  num_workers: 4
  pin_memory: true
prefetch_factor: 2

# Logging
logging:
  level: "INFO"
  log_to_file: true
  log_file: "training.log"
log_interval: 100

# Memory management
memory:
  clear_cache_interval: 1000
